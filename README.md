# Introdução

Um profissional da área de dados precisa de um ambiente de trabalho para poder executar suas análises. Claro que pode-se fazer isso localmente, já que alguns programas são leves (pandas, numpy, sklearn e etc.). Porém, para trabalhos mais robustos, que utilizam processos ETL ou ELT, com o Apache Airflow ou o EventBridge (AWS) como orquestradores, onde tem passos que utilizam banco de dados e outros serviços que compôem toda a estrutra do pipeline de dados, a criação de conteiners com a ferramenta Docker, fornece uma solução eficiente e organizada para o tratamento de cada serviço. 

Neste repositório eu apresento uma imagem criada por mim que contém alguns dos principais programas que todo cientista de dados já utilizou ou pelo menos usou alguns deles. Para executar os passos abaixo, é necessário ter o Docker instalado na máquina: <a href="https://www.docker.com/products/docker-desktop/">Download Docker</a>.

# Docker Pull e DockerHub

O DockerHub pode ser 



